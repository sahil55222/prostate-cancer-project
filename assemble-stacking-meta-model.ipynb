{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad610c6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-22T06:22:04.468936Z",
     "iopub.status.busy": "2024-06-22T06:22:04.468575Z",
     "iopub.status.idle": "2024-06-22T06:22:04.511754Z",
     "shell.execute_reply": "2024-06-22T06:22:04.510590Z"
    },
    "papermill": {
     "duration": 0.050655,
     "end_time": "2024-06-22T06:22:04.513627",
     "exception": true,
     "start_time": "2024-06-22T06:22:04.462972",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Define your custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample, target\n",
    "\n",
    "seed = 12\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Step 2: Define transformations for your custom dataset\n",
    "transform = T.Compose([\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.RandomVerticalFlip(),\n",
    "        T.RandomApply(torch.nn.ModuleList([T.ColorJitter()]), p=0.10),\n",
    "        T.RandomAffine(degrees=(-30, 30), translate=(0.2, 0.2), scale=(0.8, 1.2), shear=(-10, 10)),\n",
    "        T.Resize(256),\n",
    "        T.CenterCrop(224),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "test_transform = T.Compose([\n",
    "        T.Resize(256),\n",
    "        T.CenterCrop(224),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "\n",
    "# Step 3: Define function to get data loaders for train, validation, and test datasets\n",
    "def get_data_loaders(data_dir, test_dir, batch_size):\n",
    "    train_data = datasets.ImageFolder(os.path.join(data_dir), transform=transform)\n",
    "    test_data = datasets.ImageFolder(os.path.join(test_dir), transform=test_transform)\n",
    "\n",
    "    train_size = int(0.8 * len(train_data))\n",
    "    val_size = len(train_data) - train_size\n",
    "    train_dataset, val_dataset = random_split(train_data, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shu kiffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "import timm\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Step 4: Load pre-trained PyTorch models\n",
    "model1 = timm.create_model('deit_base_patch16_224.fb_in1k', pretrained=True)\n",
    "model2 = timm.create_model('densenet169', pretrained=True)\n",
    "\n",
    "\n",
    "\n",
    "# Modify the models for binary classification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "# Assuming 'model' is your existing model\n",
    "# Freeze all parameters in the existing model\n",
    "for param in model1.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Get the number of input features of the existing fc layer\n",
    "n_inputs = model1.head.in_features\n",
    "\n",
    "# Modify the fc layer and add more layers for reducing val loss\n",
    "n_inputs = model1.head.in_features\n",
    "model1.head = nn.Sequential(\n",
    "    nn.Linear(n_inputs, 2048),\n",
    "    nn.BatchNorm1d(2048),  # Batch Normalization\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),  # Dropout for regularization\n",
    "\n",
    "    nn.Linear(2048, 1024),  # Increase the number of neurons\n",
    "    nn.BatchNorm1d(1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "\n",
    "    nn.Linear(512, 256),\n",
    "    nn.BatchNorm1d(256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "\n",
    "    nn.Linear(256, 128),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "\n",
    "    nn.Linear(128, 64),\n",
    "    nn.BatchNorm1d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "\n",
    "    nn.Linear(64, 6)\n",
    ")\n",
    "\n",
    "model1 = model1.to(device)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "# Assuming 'model' is your existing model\n",
    "# Freeze all parameters in the existing model\n",
    "for param in model2.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Get the number of input features of the existing fc layer\n",
    "n_inputs = model2.classifier.in_features\n",
    "\n",
    "# Modify the fc layer and add more layers for reducing val loss\n",
    "model2.classifier = nn.Sequential(\n",
    "      nn.Linear(n_inputs, 2048),\n",
    "    nn.BatchNorm1d(2048),  # Batch Normalization\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),  # Dropout for regularization\n",
    "\n",
    "    nn.Linear(2048, 1024),  # Increase the number of neurons\n",
    "    nn.BatchNorm1d(1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "\n",
    "    nn.Linear(512, 256),\n",
    "    nn.BatchNorm1d(256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "\n",
    "    nn.Linear(256, 128),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "\n",
    "    nn.Linear(128, 64),\n",
    "    nn.BatchNorm1d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(64, 6)\n",
    ")\n",
    "\n",
    "model2 = model2.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 5: Load pre-trained PyTorch models\n",
    "model1_path = \"/kaggle/input/10x-deigset/models/DeiT/model_weights_epoch_199.pth\"\n",
    "model2_path = \"/kaggle/input/10x-deigset/models/DenseNet169/model_weights_epoch_199_DenseNet169.pth\"\n",
    "#model4_path = \"/kaggle/input/10x-deigset/models/pvt/model_weights_epoch_199_pvt.pth\"\n",
    "\n",
    "\n",
    "\n",
    "model1.load_state_dict(torch.load(model1_path))\n",
    "model2.load_state_dict(torch.load(model2_path))\n",
    "\n",
    "#model4.load_state_dict(torch.load(model4_path))\n",
    "\n",
    "# Set models to evaluation mode\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "\n",
    "#model4.eval()\n",
    "\n",
    "\n",
    "# Step 6: Make predictions using the pre-trained models\n",
    "def get_predictions(model, dataloader):\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs = inputs.to(device)  # Move inputs to the appropriate device\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())  # Convert to numpy array for stacking\n",
    "    return all_predictions\n",
    "\n",
    "# Step 7: Stack the predictions for meta-model training\n",
    "train_loader, val_loader, test_loader = get_data_loaders(\"/kaggle/input/10x-diaga-1200/10x/\", \"/kaggle/input/diag-10x-500/10X/Test/\", batch_size=128)\n",
    "\n",
    "# Get predictions for training and validation sets\n",
    "predictions_model1_train = get_predictions(model1, train_loader)\n",
    "predictions_model2_train = get_predictions(model2, train_loader)\n",
    "\n",
    "\n",
    "stacked_predictions_train = np.column_stack((predictions_model1_train, predictions_model2_train))\n",
    "\n",
    "# Get predictions for validation set for training the meta-model\n",
    "predictions_model1_val = get_predictions(model1, val_loader)\n",
    "predictions_model2_val = get_predictions(model2, val_loader)\n",
    "\n",
    "\n",
    "\n",
    "stacked_predictions_val = np.column_stack((predictions_model1_val, predictions_model2_val))\n",
    "\n",
    "# Extract labels for the validation set\n",
    "val_labels = []\n",
    "for _, target in val_loader.dataset:\n",
    "    val_labels.append(target)\n",
    "val_labels = np.array(val_labels)\n",
    "\n",
    "# Train the logistic regression meta-model\n",
    "meta_model = SVC() \n",
    "#meta_model = GradientBoostingClassifier() \n",
    "#meta_model = DecisionTreeClassifier()\n",
    "#meta_model = LogisticRegression(), \n",
    "#meta_model = KNeighborsClassifier() \n",
    "#meta_model = AdaBoostClassifier() \n",
    "#meta_model = XGBClassifier()\n",
    "#meta_model = GaussianNB() \n",
    "#meta_model = GradientBoostingClassifier()  \n",
    "#meta_model = RandomForestClassifier()\n",
    "meta_model.fit(stacked_predictions_val, val_labels)\n",
    "\n",
    "# Step 9: Make predictions on the test dataset\n",
    "test_predictions_model1 = get_predictions(model1, test_loader)\n",
    "test_predictions_model2 = get_predictions(model2, test_loader)\n",
    "\n",
    "\n",
    "test_stacked_predictions = np.column_stack((test_predictions_model1, test_predictions_model2))\n",
    "\n",
    "# Extract labels for the test set\n",
    "test_labels = []\n",
    "for _, target in test_loader.dataset:\n",
    "    test_labels.append(target)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "# Step 10: Use the meta-model to make predictions on the stacked test predictions\n",
    "test_meta_preds = meta_model.predict(test_stacked_predictions)\n",
    "\n",
    "# Step 11: Evaluate the accuracy of the stacked ensemble model\n",
    "ensemble_accuracy = accuracy_score(test_labels, test_meta_preds)\n",
    "print(\"Stacked Ensemble Accuracy:\", ensemble_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd5000f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e431d9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1771df9b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa831b1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4460164,
     "sourceId": 7650840,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4466952,
     "sourceId": 7660969,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4467586,
     "sourceId": 7661783,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4468939,
     "sourceId": 7663592,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4470846,
     "sourceId": 7666171,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4520391,
     "sourceId": 7735160,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4532744,
     "sourceId": 7752516,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4535401,
     "sourceId": 7756197,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4886999,
     "sourceId": 8238656,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4890325,
     "sourceId": 8243473,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4938438,
     "sourceId": 8313051,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30665,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3.009675,
   "end_time": "2024-06-22T06:22:04.735362",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-22T06:22:01.725687",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
